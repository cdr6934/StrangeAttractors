# Order and Chaos {#intro}

```{r}
library(tidyverse)
library(numDeriv)
```


This chapter lays the groundwork for everything that follows in the book. Nearly all the essential ideas, mathematical techniques, and programming tools you need are developed here. Once you’ve mastered the material in this chapter, the rest of the book is smooth sailing.

## Predictability and Uncertainty 
The essence of science is predictability. Halley’s comet will return to the vicinity of Earth in the year 2061. Not only can astronomers predict the very minute when the next solar eclipse will occur but also the best vantage point on Earth from which to view it. Scientific theories stand or fall according to whether their predictions agree with detailed, quantitative observation. Such successes are possible because most of the basic laws of nature are deterministic, which means they allow us to determine exactly what will happen next from a knowledge of present conditions.

However, if nature is deterministic, there is no room for free will. Human behavior would be predetermined by the arrangements of the molecules that make up our brains. Every cloud that forms or flower that grows would be a direct and inevitable result of processes set into motion eons ago and over which there is no possibility for exercising control. Perfect predictability is dull and uninteresting. Such is the philosophical dilemma that often separates the arts from the sciences.

One possible resolution was advanced in the early decades of the 20th century when it was discovered that the quantum mechanical laws that govern the behavior of atoms and their constituents are apparently probabilistic, which means they allow us to predict only the probability that something will happen. Quantum mechanics has been extremely successful in explaining the submicroscopic world, but it was never fully embraced by some scientists, including Albert Einstein, who until his dying day insisted that he did not believe that God plays dice with the Universe

Since the 1970s science has been undergoing an intellectual revolution that may be as significant as the development of quantum mechanics. It is now widely understood that deterministic is not the same as predictable. An example is the weather. The weather is governed by the atmosphere, and the atmosphere obeys deterministic physical laws. However, long-term weather predictions have improved very little as a result of careful, detailed observations and the unleashing of vast computer resources.

The reason for this unpredictability is that the weather exhibits extreme sensitivity to initial conditions. A tiny change in today’s weather (the  initial conditions) causes a larger change in tomorrow’s weather and an even larger change in the next day’s weather. This sensitivity to initial conditions has been dubbed the butterfly effect, because it is hypothetically possible for a butterfly flapping its wings in Brazil to set off tornadoes in Texas. Since we can never know the initial conditions with perfect precision, long-term prediction is impossible, even when the physical laws are deterministic and exactly known. It has been shown that the _predictability horizon_ in weather forecasting cannot be more than two or three weeks.

Unpredictable behavior of deterministic systems has been called chaos, and it has captured the imagination of the scientist and nonscientist alike. The word "chaos" was introduced by Tien-Yien Li and James A. Yorke in a 1975 paper entitled "Period Three Implies Chaos." The term "strange attractors," from which this book takes its title, first appeared in print in a 1971 paper entitled "On the Nature of Turbulence," by David Ruelle and Floris Takens. Some people prefer the term "chaotic attractor," because what seemed strange when first discovered in 1963 is now largely understood.

It’s not hard to imagine that if a system is complicated (with many springs and wheels and so forth) and hence governed by complicated mathematical equations, then its behavior might be complicated and unpredictable. What has come as a surprise to most scientists is that even very simple systems, described by simple equations, can have chaotic solutions. However, everything is not chaotic. After all, we can make accurate predictions of eclipses and many other things. An even more curious fact is that the same system can behave either predictably or chaotically, depending on small changes in a single term of the equations that describe the system. For this reason, chaos theory holds promise for explaining many natural processes. A stream of water, for example, exhibits smooth (_laminar_) flow when moving slowly and irregular (_turbulent_) flow when moving more rapidly. The transition between the two can be very abrupt. If two sticks are dropped side-byside into a stream with laminar flow, they stay close together, but if they are dropped into a turbulent stream, they quickly separate.

Chaotic processes are not random; they follow rules, but even simple rules can produce extreme complexity. This blend of simplicity and unpredictability also occurs in music and art. A piece of music that consists of random notes or of an endless repetition of the same sequence of notes would be either disastrously discordant or unbearably boring. Likewise, a work of art produced by throwing paint at a canvas from a distance or by endlessly replicating a pattern, as in wallpaper, is unlikely to have aesthetic appeal. Nature is full of visual objects, such as clouds and trees and mountains, as well as sounds, like the cacophony of excited birds, that have both structure and variety. The mathematics of chaos provides the tools for creating and describing such objects and sounds.

Chaos theory reconciles our intuitive sense of free will with the deterministic laws of nature. However, it has an even deeper philosophical ramification. Not only do we have freedom to control our actions, but also the sensitivity to initial conditions implies that even our smallest act can drastically alter the course of history, for better or for worse. Like the butterfly flapping its wings, the results of our behavior are amplified with each day that passes, eventually producing a completely different world than would have existed in our absence!

## Bucks and Bugs 
Enough philosophizing—it’s time to look at a specific example. This example requires some mathematics, but the equations are not difficult. The ideas and terminology are important for understanding what is to follow.

Suppose you have some money in a bank account that provides interest,
compounded yearly, and that you don’t make any deposits or withdrawals. Let’s
let $X$ represent the amount of money in your account. When the time comes for the bank to credit your interest, its computer does so by multiplying $X$ by some number. With an interest rate of 10%, the number is 1.1, and your new balance is 1.1 X. If your balance in the nth year is $X_n$  (where n is 1 after the first year, 2 after the second, and so forth), your balance in the year $n+1$ is
$$
X_{n+1}=RX_n
$$
where $R$ is equal to 1.0 plus your interest rate. ($R$ is 1.1 in this example.)

You probably know that such compounding leads to exponential growth. In
terms of the initial amount $X_0$, the amount in your account after n years is
$$
X_n = X_0R^n
$$

After 50 years at 10% yearly interest, you will have $117.39 for every dollar you initially invested. The bank can afford to do this only because of inflation and because money is loaned at an even higher interest rate.

Equation 1A is applicable to more than compound interest. It’s how many of us have our salaries determined. It also describes population growth. Imagine some species of bug that lives for a season, lays its eggs, and then dies (thus avoiding the confusion of overlapping generations). The next year the eggs hatch, and the number of bugs is some constant R times the number in the previous year. If R is less than 1, the bugs die out over a number of years; and if R is greater than 1, their number grows exponentially.

You also know that exponential growth cannot go on forever, whether it be bucks in the bank, bugs in the back yard or people on the planet. Eventually something happens, such as the depletion of resources, to slow down or even reverse growth. Mass starvation, disease, crime, and war are some of the mechanisms that limit unbridled human population growth. Thus we need to modify Equation 1A in some way if it is to model growth patterns in nature more closely.

Perhaps the simplest modification is to multiply the right-hand side of Equation 1A by a term such as (1 - X), whose value approaches 1 as X gets smaller (much less than 1) but is less than 1 as X increases. Since the population dies abruptly as X approaches 1, we must think of X = 1 as representing some large number of dollars or bugs (say a million or a billion); otherwise we would never get very far! So our modified equation, called the logistic equation, is

$$
X_{n+1}=RX_n(1-X_n)
$$

Now you’re going to get your first homework assignment. Take your pocket calculator and start with a small value of X, say 0.1. To reduce the amount of work you have to do, use a fairly large value of R, say 2, corresponding to a doubling every year. Run X through Equation 1C a few times and see what happens. This process is called iteration, and the successive values are called iterates. If you did it right, you should see that X grows rapidly for the first couple of steps, and then it levels off at a value of 0.5. The first few values should be approximately 0.1, 0.18, 0.2952, 0.4161, 0.4859, 0.4996, and 0.5. Compare your results with the unbounded growth of Equation 1A.

You might have predicted the above result, if you had thought to set Xn+1 equal to Xn in Equation 1C and solved for Xn. This value is called a fixed-point solution of the equation, because if X ever has that value, it remains fixed there forever. Such a fixed-point solution is sometimes called a point attractor, because every initial value of X between 0 and 1 is attracted to the fixed point upon repeated iteration of Equation 1C. Try initial values of X = 0.2 and X = 0.8. A fixed point is also called a critical point, a singular point, or a singularity.

If you’re curious, you might wonder what happens if you start with a value of X less than 0, such as -0.1, or greater than 1, such as 1.1. You should verify that the iterates are negative and that they get larger and larger, eventually approaching minus infinity. We say that the solution is unbounded and that it attracts to infinity. Thus the values of X = 0 and X = 1 are like a watershed. Between these values the solution is bounded, and outside these values it is unbounded.

The region between X = 0 and X = 1 is called a basin of attraction because it resembles a bathroom basin in which drops of water find their way to the drain from wherever they start. X = 0 is also a fixed point, but it is unstable because values either slightly above or slightly below zero move away from zero. Such an unstable fixed point is sometimes called a repellor. Chaos can result when two or more repellors are present; the iterates then bounce back and forth like a baseball runner caught in a squeeze play.

See what happens if you substitute X = 0 or X = 1 into the logistic equation. As a check on your calculations, or in case you didn’t do your homework, Table 1-1 shows the successive iterates of X for each of the cases we have discussed.

An equation, such as the logistic equation, that predicts the next value of a quantity from the previous value is called an iterated map because it is like a road map in which each point on the earth is mapped to a corresponding point on a piece of paper. The logistic equation is a one-dimensional map because the various X values can be thought of as lying along a straight line that stretches from minus infinity to plus infinity. Each iteration of the map moves every point along the line to a new position on the line. For the example above with R = 2, all the points between X = 0 and X = 1 walk toward X = 0.5, where they stop and remain. Other points run faster and faster toward the end of the line that stretches to minus infinity.

The logistic equation is an example of a quadratic iterated map, so called because if you multiply out the right-hand side of Equation 1C, it has not only a linear term RXn but also a quadratic (squared) term $-RX_n^2$. Quadratic maps are noninvertable because you can find Xn+1 from Xn , but can’t go backward because there are two values of Xn  that produce the same Xn+1, and there is no way of knowing from which it came. For example, Table 1-1 shows that X0 = 0.2 and X0 = 0.8 both produce X1 = 0.32. These are the two roots of the quadratic equation that you get if you try to solve for Xn in Equation 1C in terms of Xn+1.

The graph of Xn+1 versus Xn  is a curve called a parabola. Because a parabola is not a straight line, the map is said to be nonlinear. Chaos and strange attractors require nonlinearity. The interesting and surprising behavior of nonlinear iterated maps is the basis for much of this book.

The first surprising result occurs if you iterate Equation 1C with R = 3.2 and an initial value of X in the range of 0 to 1. After a few iterations the solution will alternate between two values of approximately 0.5130 and 0.7995. This is called a period-2 limit cycle. Like the fixed point, the limit cycle is another type of simple attractor. It is sometimes called a periodic or cyclic attractor.

It’s not hard to see how cyclic behavior might arise in nature. If the population of beetles grows too large, they deplete the plants on whom they depend for food. With too few plants, the beetles die out, allowing the number of plants to recover, leading to the next cycle of beetle growth, and so forth.

Increase R a bit more to 3.5, and repeat the calculation. The result is a period 4 limit cycle with four values of approximately 0.5009, 0.8750, 0.3828, and 0.8269. If you keep increasing R by ever smaller amounts, the period of the limit cycle doubles repeatedly, finally reaching chaotic behavior (an infinite period) at about R = 3.5699456. This value is sometimes called the Feigenbaum point, after Mitchell J. Feigenbaum, a contemporary mathamatician who discovered many of the inter esting properties of one-dimensional maps.

When chaos occurs, the successive iterates fluctuate in an apparently random and irreproducible manner. The chaotic behavior persists up to R = 4 except for an infinite number of small periodic windows. For R greater than 4, the solution is unbounded, and the iterates attract rapidly to minus infinity.

The behavior described above can be summarized in a bifurcation diagram, as shown in Figure 1-1, in which the limiting iterated values of the logistic equation, after discarding the first few hundred iterates, are plotted for a range of R from 2 to 4. This plot is called the Feigenbaum diagram, and it resembles a tree on its side. ("Feigenbaum," appropriately but coincidentally, is German for "fig tree.") You see the fixed-point solution for R less than 3, the period-doubling route to chaos, and the periodic windows at large $R$. The chaotic regions toward the right side of the figure are characterized by values of $X$ that span a wide range and eventually fill the region densely with points.

```{r}

logistic.map <- function(r, x, N, M){
  ## r: bifurcation parameter
  ## x: initial value
  ## N: number of iteration
  ## M: number of iteration points to be returned
  z <- 1:N
  z[1] <- x
  for(i in c(1:(N-1))){
    z[i+1] <- r *z[i]  * (1 - z[i])
  }
  ## Return the last M iterations 
  z[c((N-M):N)]
}

## Set scanning range for bifurcation parameter r
my.r <- seq(2, 4, by=0.003)

Orbit <- sapply(my.r, logistic.map,  x=0.1, N=1000, M=300)
Orbit <- as.vector(Orbit)
r <- sort(rep(my.r, 301))

plot(Orbit ~ r, pch=".")

#data.frame(xn) %>% ggplot(aes(x,y)) + geom_point()
```


Each period doubling is called a _bifurcation_ because a single solution splits into a  pair of solutions. These splittings are called pitchfork bifurcations for obvious reasons. Note the period-3 window at about R = 3.84. The period-3 region begins abruptly when R is increased slightly from within the chaotic region to its left in what is called a tangent or saddle-node bifurcation. Careful inspection of the period-3 window shows that it also undergoes a period-doubling sequence at about R = 3.85. Solutions with every period can be found somewhere between R = 3 and R = 4.


Successive period doublings occur with ever-increasing rapidity as one moves from left to right in Figure 1-1. The ratio of the width of each region to the width of the previous region approaches a constant equal to 4.669201660910..., called the Feigenbaum number. Even more remarkable is that this number arises in many different chaotic systems in nature as well as in the solutions of equations. The universality of the Feigenbaum number in chaos is reminiscent of the ubiquity of the number p in Euclidean geometry

With R = 4 the solutions occupy the entire interval from X = 0 to X = 1. Eventually X takes on a value arbitrarily close to any point in that interval (a characteristic called topological transitivity). Curiously, however, infinitely many initial values of X don’t lead to a chaotic solution even for R = 4. For example $X_0 = 0.5$ and $X_0 = 0.75$ lead to unstable fixed points, while $X_0 = 0.345491...$ and $X_0 = 0.904508...$ produce an unstable period-2 limit cycle. By nstable we mean that if the initial values are wrong by even the slightest amount, successive iterates will wander ever farther away.

Even though there are infinitely many nonchaotic initial values between zero and one, the chance that you will find one by randomly guessing is negligible. For every such value, there are infinitely many others that produce chaos. Such a seemingly paradoxical entity is an example of a Cantor set, named after the 19th century Russian-born German mathematician Georg Cantor who is often credited with developing a mathematically rigorous concept of infinity.

A Cantor set contains infinitely many members (in fact, uncountably infinitely many), but its members represent a zero fraction of the total! For example, infinitely many points are required to cover completely the circumference of a circle, but this number of points doesn’t even begin to cover its interior. Such a collection (or set) of points, although infinite in number, is said to comprise a set of measure zero, because the points fill a negligible portion of the plane. An attractor is a set of measure zero, but its basin of attraction has a nonzero measure.

Few people would have guessed that such complexity could arise from such underlying simplicity. Furthermore, the logistic equation is only the simplest of an endless variety of equations that can exhibit chaos. It is this dichotomy of simplicity and complexity that makes chaos beautiful to the mathematician and artist alike. In the bifurcation diagram of the logistic equation, we have something with aesthetic appeal, and it came from a simple quadratic equation!

## The Butterfly Effect

If our goal is to seek chaotic behavior in the solution of equations, we need a simple way to test for chaos. For this purpose we use the fact that chaotic processes exhibit extreme sensitivity to initial conditions, in contrast to regular processes in which different starting points usually converge to the same sequence of points on a simple attractor.

Suppose we iterate the logistic equation with two initial values of $X$ that differ by only a tiny amount. Think of these values as representing two states of the atmosphere that differ only by the flapping of the wings of a butterfly. If successive iterates are attracted to a fixed point as they are for $R = 2$, the difference between the two solutions must get smaller and smaller as the fixed point is approached. A similar thing happens for a limit cycle. The difference between the two solutions will on average decrease exponentially.

If the solution is chaotic, as is the logistic equation for R = 4, the successive iterates for the two cases initially on average get farther apart; the difference usually increases exponentially. If the difference doubles on average with every iteration, we say the Lyapunov exponent is 1. If it is reduced by half, we say the Lyapunov exponent is -1. The name comes from the late-19th-century Russian mathematician Aleksandr M. Lyapunov (sometimes transliterated Liapunov or Ljapunov).

You can think of the Lyapunov exponent as the power of 2 by which the difference between two nearly equal X values changes on average for each iteration. Thus the difference between the values changes by an average of 2 L for
each iteration. If L is negative, the solutions approach one another; if L is positive,
we have sensitivity to initial conditions and hence chaos.

One way to detect chaos is to iterate the equation with two nearly equal initial values and see if, after many iterations, the values are closer together or farther apart. Another way is to make use of a principle of calculus that says that the difference in the solutions after one iteration divided by the difference before the iteration, provided the difference is small, is equal to the derivative of the equation for the map, which for the logistic equation is

$$
\frac{\Delta X_{n+1}}{\Delta X_n}=R(1-2X_n)
$$
where $\Delta X$ is the difference between the two values of X. In Equation 1D, $\Delta X_n$ is the difference in the X values after n iterations, and $\Delta X_{n+1}$ is the difference after n+1 iterations.

Since DX increases by the factor on the right of Equation 1D for each iteration, the proper way to calculate the average is to start with a value of 1 and multiply it repeatedly by the right-hand side of Equation 1D at each iteration, then divide the result by the number of iterations, and finally take the logarithm to the base 2 of the absolute value of the result to get the Lyapunov exponent. If you prefer an equation, the preceding description is equivalent to


$$
L=\Sigma\frac{\log_2|R(1-2X_n)|}{N}
$$
where the vertical bars mean that you are to disregard the sign of the quantity inside, and $\Sigma$ means to sum the quantity to its right from a value of $n = 1$ to a value of $n = N$, where N is some large number. The larger the value of $N$, the more accurate the estimate of $L$.

Suppose you knew the value of X to within 0.01 for an iterated map with L = 1. After one iteration the uncertainty would be about 0.02, and after two iterations the uncertainty would be about 0.04, and so forth. After about seven iterations, the error would exceed 1, and your prediction would be totally worthless. If the X values are expressed as binary numbers, each iteration would result in throwing away the rightmost (least significant) binary digit (bit). Thus the units of L are bits per iteration. Sometimes L is expressed in terms of the natural logarithm (base e) rather than $log_2$. The Lyapunov exponent is the rate at which information is lost when a map is iterated.

It is as if a succession of cartographers each copied maps from one another, but every time one was copied it was only half as accurate as the previous one. If the original map were accurate to 1%, the next copy would be accurate to 2%, and the seventh generation copy would bear no relation to the original. If the Lyapunov exponent were -1, one bit of information would be gained at each iteration. Even a completely unknown initial condition would eventually be perfectly accurate as it approached the known fixed point or limit cycle. Unfortunately, negative Lyapunov exponents are not the rule in cartography; otherwise all our maps would be selfcorrecting!


TODO: Create the lyapunov exponent
```{r}
lyap <- function(a,trans=300,num=1000){
    x0 <- runif(1)
    for(time in 1:trans){
        x1 <- f(x0,a=a);x0 <- x1
    }
    sl <- 0
    for(time in 1:num){
        x1 <- f(x0,a=a);x0 <- x1
        sl <- sl+log(abs(grad(f,x1,a=a)))
    }
    sl/num
}

a <- seq(0.5,4,length=100)
ly <- sapply(a,lyap)
plot(a,ly,t="l")

```

Figure 1-2 shows the Lyapunov exponent for the logistic equation using values of R from 2 to 4. The Lyapunov exponent is 1.0 at $R = 4$ because that value causes the interval of X from 0 to 1 to be mapped backed onto itself with a single fold at $X = 0.5$. Thus information is lost at a rate of 1 bit per iteration, because each iterate has two possible predecessors. You can also see some of the periodic windows where L dips below zero toward the right edge of the plot. Also note that L is zero wherever a bifurcation occurs, for example at $R=2$. At these points the solution is fraught with indecision over which branch to take, and the initial uncertainty persists forever, neither increasing nor decreasing.

## The Computer Artist

By now you have probably surmised that the operations we have described are best carried out by a computer. The equations are simple, but they must be applied repeatedly. This is precisely the kind of task at which computers excel.

There are dozens of computer types and programming languages to choose from. Currently the most popular computers are those based on the IBM PC running the MS-DOS or IBM-DOS operating system (hereafter simply called DOS). The most widely available programming language is BASIC (Beginner’s All-purpose Symbolic Instruction Code), which usually comes bundled with the operating system software included with the computer. A version of BASIC called QBASIC has been included with DOS since version 5.0. BASIC may not be the most advanced computer language, but it is one of the easiest to learn and to use, its commandsare close to ordinary English, and it is more than adequate for our purposes. Furthermore, modern versions of BASIC compare favorably with the best of the other languages.

The American National Standards Institute (ANSI) has established a standard for the BASIC language, but it is somewhat limited, and most versions of BASIC have many additions and embellishments. We will intentionally use a primitive dialect to ensure compatibility with most modern implementations and to simplify the translation into incompatible versions. In particular, the programs in this book should run without modification under Microsoft BASICA, GW-BASIC, QBASIC, QuickBASIC, VisualBASIC for MS-DOS; Borland International Turbo BASIC (no longer available); and Spectra Publishing PowerBASIC on IBM PCs or compatibles. You will be happiest using a modern compiled BASIC such as VisualBASIC or PowerBASIC on a fast computer with a math coprocessor.

Appendix C includes information on translating the computer programs into other, partially incompatible dialects of BASIC, as well as source code for use with VisualBASIC for Windows and Microsoft QuickBASIC for the Macintosh. Appendix D contains a translation into Microsoft QuickC. The BASIC programs use line numbers, which have been obsolete since the mid-1980s, but they are harmless, and they provide a convenient way to reference lines of the program and to indicate where in the program a change is to be made.

If you follow sequentially through this book, you will need to add and change a only few lines of the program as you meet each new idea. Your program will gradually grow more versatile as you work through the book. In the end you will have a powerful program that can reproduce all the examples in this book as well as an endless variety of new ones. Hence you should avoid the temptation to eliminate or to change the line numbers, at least until you have a fully functional program. You may prefer to jump to Appendix B where you will find the complete final program, which is also provided on the accompanying disk along with source listings in BASIC, Microsoft QuickC, Borland Turbo C++ and a ready-to-run executable version of the program.

If you are an experienced programmer, you might ridicule some of the quaint program listings. Many powerful programming structures such as block IF statements, DO LOOPs, and callable subroutines with local variables that produce beautifully structured programs are now standard, but they have been avoided to allow backwards compatibility with more primitive versions of BASIC. They also often impose a small speed penalty. The dreaded GOTO statement has been used primarily to bypass blocks of code in deference to BASIC versions that don’t support block IF statements. Lines of the program that are bypassed by a GOTO are usually indented. Blocks of the program contained within FOR...NEXT loops have also been indented. In the interest of structure and simplicity, the programs have been written using numerous small modular subroutines, each with a single entry point beginning with a comment line, and a single exit point containing a RETURN statement, albeit with global variables. The individual subroutines are separated with blank lines. It should be relatively easy for an experienced programmer to rewrite the program in a more modern format.

The program listing PROG01 iterates the logistic equation for R = 4 with an initial value of X = 0.05 and makes a graph of each iterate versus its predecessor. The program looks more complicated than it actually is because the various operations have been relegated to subroutines to provide a template for the more versatile cases to follow.




```{r}
data <- seq(-10,10,1)

parabola <- function(x) {4*x*(1-x)}

ly <- sapply(data,parabola)

plot(data, ly,t = "l")

```


1000 REM LOGISTIC EQUATION
1010 DEFDBL A-Z 'Use double precision
1030 SM% = 12 'Assume VGA graphics
1190 GOSUB 1300 'Initialize
1200 GOSUB 1500 'Set parameters
1210 GOSUB 1700 'Iterate equations
1220 GOSUB 2100 'Display results
1230 GOSUB 2400 'Test results
1240 ON T% GOTO 1190, 1200, 1210
1250 CLS
1260 END

1300 REM Initialize
1320 SCREEN SM% 'Set graphics mode
1350 WINDOW (-.1, -.1)-(1.1, 1.1)
1360 CLS
1420 RETURN

1500 REM Set parameters
1510 X = .05 'Initial condition
1560 R = 4 'Growth rate
1570 T% = 3
1590 LINE (-.1, -.1)-(1.1, 1.1), , B
1630 RETURN

1700 REM Iterate equations
1720 XNEW = R * X * (1 - X)
2030 RETURN

2100 REM Display results
2300 PSET (X, XNEW) 'Plot point on screen
2320 RETURN

2400 REM Test results
2490 IF LEN(INKEY$) THEN T% = 0 'Respond to user key stroke
2510 X = XNEW 'Update value of X
2550 RETURN


If, when you first run the program, your computer reports an error, it is probably
in one of the following lines:

Line 1010: Be sure your version of BASIC supports double-precision (four-byte)
floating-point variables. If it doesn’t, you may omit this line, but then you probably
will have to change the 4 in line 1560 to 3.99999 to avoid overflow resulting from
round-off errors. With modern versions of BASIC and a computer with a math
coprocessor, there is no penalty, and considerable advantage, in using double
precision. Because of the finite precision of computer arithmetic, all cases will
eventually repeat, but with double precision the average number of iterations
required before this happens is acceptably large.

Line 1320: Either your version of BASIC doesn’t require this command or your
computer or compiler doesn’t support VGA graphics. Try reducing the 12 in line 1030
to a lower number until you find one that works. If none works, try eliminating line
1320 altogether.

Line 1350: The WINDOW command defines the coordinates of the lower-left
and upper-right corners of the graphics window for subsequent PSET and LINE
commands. If your version of BASIC doesn’t support this command, you must delete
this line and convert all the parameters in the PSET and LINE commands to address
screen pixels. In this case try replacing line 2300 with PSET (200 * X, 200 - 200 * XNEW). One advantage of using the WINDOW command is that when a version of BASIC
comes along that supports higher screen resolutions, the program can be easily
recompiled to take advantage of it.

Other errors: Look carefully for typographical errors, or consult your BASIC
manual to determine compatibility.

The correct program should produce a plot of the logistic parabola, as shown
in Figure 1-3. Try different initial values of X (line 1510) and different values of R (line
1560) to confirm the behavior predicted for the logistic equation.

```{r}

```


**TODO: Insert parabola from PROG01**

The logistic parabola comes from a chaotic solution, but it doesn’t look very complicated, and it would hardly qualify as art. With one small change we can make things more interesting and, at the same time, illustrate sensitivity to initial conditions. Instead of plotting each iterate versus its immediate predecessor, we could plot it versus its second or third or fourth predecessor. Let’s save the last 500 iterates and provide the option to plot X versus any one of them

The changes that you need to make in the program PROG01 to accomplish this are shown in the listing PROG02. You can either go through the program and change or add lines as necessary or type the listing and save it in ASCII format and then use the MERGE command supported by many (mostly old) versions of BASIC to update the previous version of the program.


1000 REM LOGISTIC EQUATION (5th Previous Iterate)
1020 DIM XS(499)
1040 PREV% = 5 'Plot versus fifth previous iterate
1580 P% = 0
2210 XS(P%) = X
2220 P% = (P% + 1) MOD 500
2230 I% = (P% + 500 - PREV%) MOD 500
2300 PSET (XS(I%), XNEW) 'Plot point on screen


If you set PREV% = 1 in line 1040, the result is the same as for PROG01. However, if you set PREV% equal to 2, you see the logistic parabola change into a curve with two humps. Each time you increase PREV% by 1, you double the number of humps in the curve. Thus PREV% = 5 results in 16 oscillations, as shown in Figure 1-4.

```{}



```

**TODO Insert picture her**


Figure 1-4 provides a good graphical illustration of the sensitivity to initial conditions. The horizontal axis represents all possible initial conditions from zero to one. The vertical axis shows the value from zero to one corresponding to each initial condition after five iterations. It’s not hard to see that two nearby points on the horizontal axis usually translate into two very different values along the vertical axis after five iterations. Try using PREV% = 10, and convince yourself that information about the initial condition is almost completely lost after ten iterations.

This exercise provides a good insight into the way a strange attractor is formed geometrically. The logistic parabola, which began as a line (a onedimensional object), is stretched and folded with each iteration, eventually filling the entire plane (a two-dimensional object) after many iterations. Perhaps it reminds you of those taffy machines that repeatedly stretch and fold the taffy, causing two nearby specks in the taffy after a while to be nowhere near one another. On average the distance between the specks initially increases at an exponential rate.

You should be able to think of many other examples of sensitivity to initial conditions. When you stir your coffee to mix in the cream, you’re relying on a chaotic process. Two sticks dropped into the water close together just above a waterfall eventually end up far apart. Try laying two identical garden hoses side by side, and turn on the water in each one at the same time without holding the ends. Chaotic processes are all around us. Their mathematical solutions usually produce chaotic strange attractors, whose diversity and beauty we are about to explore.


